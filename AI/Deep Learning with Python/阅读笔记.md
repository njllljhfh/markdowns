# Deep Learning with Python

> Deep Learning with Python 阅读笔记



## 一、深度学习的基本概念

### 1、什么是机器学习？

#### 1.1、 人工智能，机器学习和深度学习

人工智能，机器学习和深度学习的关系，如图。



![人工智能，机器学习和深度学习的关系](.\image\figure-1.1.png)

##### 1.1.1、人工智能

symbolic AI：

- 从1950s 到 1980s 末期
- 适合：定义较清晰，逻辑较清楚的问题，例如国际象棋。
- 不适合：复杂的问题，例如图像分类，语音识别，翻译等。



##### 1.1.2、机器学习

Analytical Engine：

- 1830s 和1840s



symbolic AI  和 机器学习的区别，如图。

![figure-1.2](.\image\figure-1.2.png)



##### 1.1.3、从数据中学习表示（representations）

要做机器学习，我们需要三件事:

1. 输入数据
2. 预期的输出结果
3. 一个衡量算法是否工作良好的方法



##### 1.1.4、“深度”在深度学习中的意思





##### 1.1.5、通过三张图理解深度学习是如何工作的

- Layer(data transformation)：层（layered representations）
- Loss function：损失函数（目标函数）
- Loss score：损失值
- Optimizer：优化器



深度学习是一个从数据中学习表示  （representations ）的数学框架。



##### 1.1.6、目前深度学习取得的成就

pass



##### 1.1.7、不要相信短期炒作

第一次兴起到衰退：symbolic AI, 1960s -1970s  

第二次兴起到衰退：1980s - the early 1990s  , symbolic AI, expert systems  



##### 1.1.8、人工智能的承诺

pass

---



#### 1.2、深度学习之前:机器学习的简史

##### 1.2.1、概率建模(Probabilistic modeling)

应用：

- 朴素贝叶斯算法（Naive Bayes algorithm）
  - 朴素贝叶斯分类器（Naive Bayes classifiers）：假设所有输入数据的特征都是独立的（Naive Bayes is a type of machine-learning classifier based on applying Bayes’ theorem while assuming that the features in the input data are all independent (a strong, or “naive” assumption, which is where the name comes from)  .)
    - 一个密切相关的模型是 逻辑回归（logistic regression (简称：logreg)）：logreg  是一个分类算法，而不是回归算法。



##### 1.2.2、早期的神经网络

应用：

- 对手写的数字进行分类：该神经网络被命名为 LeNet，1989 ，卷积神经网络（convolutional neural networks），反向传播（backpropagation）。



##### 1.2.3、核方法（Kernel methods）

核方法：核方法是一组分类算法，其中最出名的是 **支持向量机**（support vector machine (SVM)）。

支持向量机：

- 支持向量机的目的是通过在属于两种不同类别的两组点之间寻找好的决策边界(见图1.10)来解决分类问题。

  ![figure-1.10](.\image\figure-1.10.png)



超平面（hyperplane）

核函数（kernel function）

特征工程（feature engineering）



##### 1.2.4、决策树、随机森林和梯度增强机

梯度增强机：

- 一种通过迭代训练新模型来改进任何机器学习模型的方法，这些新模型专门解决以前模型的弱点。
- 它可能是当今处理非感知数据最好的算法之一，即使不是最好的。



##### 1.2.5 回到神经网络

2011

2012, ImageNet  

deep convolutional neural networks (convnets)   



##### 1.2.6 深度学习的不同之处是什么？

​		What is transformative about deep learning is that it allows a model to learn all layers of representation jointly, at the same time, rather than in succession (greedily, as it’s called). With joint feature learning, whenever the model adjusts one of its internal features, all other features that depend on it automatically adapt to the change, without requiring human intervention. Everything is supervised by a single feedback signal: every change in the model serves the end goal. This is much more powerful than greedily stacking shallow models, because it allows for complex, abstract representations to be learned by breaking them down into long series of intermediate spaces (layers); each space is only a simple transformation away from the previous one.  

​		These are the two essential characteristics of how deep learning learns from data: the ***incremental, layer-by-layer way in which increasingly complex representations are developed***, and the fact that ***these intermediate incremental representations are learned jointly***, each layer being updated to follow both the representational needs of the layer above and the needs of the layer below. Together, these two properties have made deep learning vastly more successful than previous approaches to machine learning.  



##### 1.2.7 现代机器学习

热门方法：

- gradient boosting machines: for shallowlearning problems. (重要)

- deep learning: for perceptual problems. (重要)



热门库：

- XGBoost library (重要)

- Keras library (重要)



热门语言：

- Python
- R. 

---



#### 1.3 为什么深度学习? 为什么是现在?

LSTM (Long short - term Memory)算法，1997  

在20世纪90年代和21世纪初，真正的瓶颈是数据和硬件



##### 1.3.1、硬件

从1990年到2010年，现有的cpu速度提高了大约5000倍。

2007, NVIDIA launched CUDA (https://developer.nvidia.com/about-cuda)  



##### 1.3.2、数据

ImageNet dataset  



##### 1.3.3、算法

The key issue was that of ***gradient propagation*** through deep stacks of layers.   

- 更好的神经层激活函数（activation functions）
- weight-initialization schemes  
- optimization schemes  



##### 1.3.4、新一轮的投资浪潮



##### 1.3.5、深度学习的民主化



##### 1.3.6、深度学习会持续下去吗?

2017年，处于sigmoid curve的初段（快速增长期）

---



### 2、在我们开始之前:神经网络的数学构建模块

#### 2.1、初识神经网络

densely connected (also called fully connected) neural layers.  



#### 2.2、神经网络的数据表示

##### 2.2.5 关键属性

张量由三个关键属性定义:

- *Number of axes (rank)*
- *Shape*
- *Data type* (usually called dtype in Python libraries)：Note that string tensors don’t exist in Numpy (or in most other libraries), because tensors live in preallocated, contiguous memory segments: and strings, being variable length, would preclude the use of this implementation.