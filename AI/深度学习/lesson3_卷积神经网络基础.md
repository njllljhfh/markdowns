# lesson3_卷积神经网络基础

> autor: njl
>
> from：小象学院



## 卷积神经网络

四大要件：

- **卷积层（Conv Layer）**  【核心】
- **非线性激励层（Non-linear Layer）**  【核心】
- 池化层（Pooling Layer）
- 输出层（Fully-connected Layer）

---

分类

对图像做分类：比如识别一个图像，给他一个label

对像素做分类：检测任务、分割任务。

在整个前馈的过程中，至少有一个由卷积来操作进行传播的层。

---

三大特性：

- 局部感受野（Locally Receptive Area）
- 权值共享（Shared Weights）
- 下采样（Spatial or Temporal Sub-sampling）

--> 对平移/缩放/扭曲的不变形

---

- 神经网络的卷积层部分，可以认为是一个 **【特征提取器】**

---

- 每个卷积核可以认为它会专注于输入的特征图的一个特征。将很多个卷积核得到信息融合起来，就得到了一个更高维的表达。
- 下一层特征图的个数，与上一层的卷积核个数是匹配的（即，上一层如果有16个卷积核，那么下一层就会输出16个特征图）。

---

卷积核大小：

- 奇数偶数选择：一般为奇数，满足对称。
- 大小选择：根据输入数据，根据图像特征。
    - VGG：用的3×3的卷积核。
- 厚度（深度）确定：与输入数据一致。

- 覆盖范围：根据输入数据，根据图像特征。

---

卷积层--关键参数：

- 步长（stride）：对输入特征图的扫描间隔
- 边界扩充（pad）：在卷积计算过程中，为了允许边界上的数据也能作为中心参与卷积运算，将边界假装延伸。
    - 扩充目的：确保卷积后，特征图尺度一致。
    - 方法：卷积核的宽度为 $2i+1$ , 添加pad宽度为 $i$ 。
        - 3×3的卷积核一般做zero-padding就可以。

---

卷积神经网络需要哪些额外的功能（功能层）？

- 非线性激励：卷积是线性运算。通过非线性激励，来增加非线性描述能力。
    - 例如：ReLU函数。
- 降维：特征图稀疏，减少数据运算量，保持精度。
    - （例如：池化层）在全连接层前面把维度降下来，防止全连接层参数过多，造成过拟合。
        - 降维方式主要有两种：
            1. max池化（适用范围更广）：比如在2×2的范围内取最大值。
            2. average池化：比如在2×2的范围内取算数平均值。
- 归一化：特征scale保持一致。
    - 像素上的归一化、空间上的中心化。
    - 其实很多时候，在数据输入神经网络前的与处理中，就已经把归一化做掉了。
    - **归一化层要放在非线性激励层之前**。
        - Batch Normalization（BN），即，批量归一化：
            1. 方式：根据每一批（mini batch）的数据来进行归一化。BN在训练中有参数需要优化。
            2. 批量归一化的原因：特征数Scale不一致。
            3. 好处：加速训练，提高精度。
        - Local Response Normalization（近邻归一化）：
            1. 方式：有标准的计算公式。训练中没有训练参数。
- 区域分割：不同区域进行独立学习。
    - 希望独立对某些区域进行单独学习，不希望一个卷积核对整个特征图进行特征的提取或编码，则可以把一层来进行切分，这样可以学习多套参数，得到更强的特征描述能力。
    - 很多情况下都不需要切分层。
- 区域融合：对分开的区域合并，方便信息融合。
    - 对独立进行特征学习的分支进行融合，构建高效而精简的特征组合。
- 曾维：增加图片生成或探测任务中空间信息（上采样？反卷积？）。

---

