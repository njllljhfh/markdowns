# lesson1_深度学习总体介绍与神经网络入门

> autor: njl
>
> from：小象学院



主流框架：

- TensorFlow（首选）
    - 维护团队：Google
    - 静态框架
- PyTorch
    - 维护团队：Facebook
    - 动态框架

---

- CUDA

---

卷积核

边界算子



- CNN：卷积神经网络

---

分类

回归

---



## 线性回归

梯度下降



非线性**激励函数**：

- sigmoid（现在用的不多了）

- tahn（现在用的不多了）
- ReLU（Rectified linear unit）:
- Leaky ReLU



SGD随机梯度下降

---

### 逻辑回归

- 其实不是回归问题，是分类问题

---

### mac如何用外置显卡，进行DL训练？？？

---

### 顶会

**vision**: 

- CVPR,
-  ICCV,
-  ECCV。

**nlp**: 

- ACL, 
- EMNLP。

**general DL/ML**: 

- NIPS, 
- ICML, 
- ICLR。

**AI**: （论文的质量不如上面的好）

- AAAI, 
- IJCAI。

---

### 数学知识

[机器学习数学知识](https://www.jiqizhixin.com/articles/2017-02-25)

1. 线性代数：我的一个同事 Skyler Speakman 最近说过，「线性代数是 21 世纪的数学」，我完全赞同他的说法。在机器学习领域，线性代数无处不在。主成分分析（PCA）、奇异值分解（SVD）、矩阵的特征分解、LU 分解、QR 分解、对称矩阵、正交化和正交归一化、矩阵运算、投影、特征值和特征向量、向量空间和范数（Norms），这些都是理解机器学习中所使用的优化方法所需要的。令人惊奇的是现在有很多关于线性代数的在线资源。我一直说，由于大量的资源在互联网是可以获取的，因而传统的教室正在消失。我最喜欢的线性代数课程是由 MIT Courseware 提供的（Gilbert Strang 教授的讲授的课程）：http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/

2. 概率论和统计学：机器学习和统计学并不是迥然不同的领域。事实上，最近就有人将机器学习定义为「在机器上做统计」。机器学习需要的一些概率和统计理论分别是：组合、概率规则和公理、贝叶斯定理、随机变量、方差和期望、条件和联合分布、标准分布（伯努利、二项式、多项式、均匀和高斯）、时刻生成函数（Moment Generating Functions）、最大似然估计（MLE）、先验和后验、最大后验估计（MAP）和抽样方法。

3. 多元微积分：一些必要的主题包括微分和积分、偏微分、向量值函数、方向梯度、海森、雅可比、拉普拉斯、拉格朗日分布。
4. 算法和复杂优化：这对理解我们的机器学习算法的计算效率和可扩展性以及利用我们的数据集中稀疏性很重要。需要的知识有数据结构（二叉树、散列、堆、栈等）、动态规划、随机和子线性算法、图论、梯度/随机下降和原始对偶方法。
5. 其他：这包括以上四个主要领域没有涵盖的数学主题。它们是实数和复数分析（集合和序列、拓扑学、度量空间、单值连续函数、极限）、信息论（熵和信息增益）、函数空间和流形学习。