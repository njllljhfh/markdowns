# 一、数学基础1-数学分析

 概率（测度？）

sigmoid函数，Logistic函数

概率密度函数（Probability Density Function)，简称PDF

累计分布函数（Cumulative Distribution Function），简称CDF

古典概型

信息熵

贝叶斯学派 和 频率学派

泊松分布、二项分布、两点分布。（都是离散的）

均匀分布、指数分布、正态分布、二元正态分布

泰勒展开式

指数族分布：泊松分布、二项分布、两点分布、高斯分布



---



数据收集 → 数据清洗 → 特征工程 → 数据建模

特征决定模型的的上限（理解业务）

LDA：线性判别分析



`加速度` 的方向总是指向轨迹曲线凹的一侧。



O(lnN!)  约等于 O(NlnN)

## Sigmoid/Logistic函数

$$
f(x) = \cfrac{1}{1+e^{-x}}
$$



# 二、数学基础2-数理统计与参数估计

- 期望（均值）
- 方差
- 标准差
- 协方差

- 所谓的X、Y`不相关`，是一维上的，X、Y没有`线性相关`性，协方差=0，Pearson相关系数=0

- X、Y独立，则X、Y一定不相关，反之不一定成立。

- 协方差矩阵，是对称矩阵
- 正交矩阵
- X的k阶原点矩：$E(X^{k})$，X的`1阶原点矩`就是`期望`
- X的k阶中心距： $E\{[X-E(X)]^{k}\}$，X的`2阶中心距`就是`方差`

- 变异系数：`标准差`与`期望`的比值称为`变异系数`，记为CV
- 偏度
- 超值峰度
- 峰度x
- 切比雪夫不等式
- 大数定理
- 中心极限定理
- k阶样本原点矩
- k阶样本中心距
- 样本方差
- 矩估计：用样本去估计总体

- 最大似然估计
- 对数似然函数

- 矩估计和最大似然估计的思想不一样，但是结论是一样的。
- 偏差
- 欠拟合
- 过拟合



# 三、数学基础3-矩阵和线性代数

- 范德蒙行列式
- 插值（自己找教材看）
- 保证样本点距离拟合曲线不是太远
- （条件）概率转移矩阵
- 全概率公式
- 特征值
- 特征向量（特征向量一定是非零的）
- 一个 `m×n的矩阵A` 乘以 一个 `n维列向量x` 得到一个 `m维的列向量`，实际上是将一个n维空间上的一个点x 通过矩阵A 映射成了一个m维空间上的点y。如果m=n则是在同一个维度空间中的映射。
- 分治算法、动态规划（自己去了解）
- 向量组等价
- 系数矩阵
- 线性表出
- 正交阵，正交变换（正交变换不改变向量长度）
- 矩阵的迹
- 对称阵
- 对角化，合同变换
- 白化/漂白whitening（经过该预处理后，再经过其他处理，最后的结果不一定会理想，有可能还不如不白化）
- 中心化
- 正定阵，半正定阵，负定阵，半负定阵（`正定`，其实就 是`正数` 在 `n维` 上的推广）。正定阵的特征值都为正数
- 向量对向量求导
- 超越矩阵 
- 行列式等于0的方阵是奇异矩阵，不等于0是非奇异矩阵
- QR分解（计算n阶方阵的特征值）
- 隐特征，LFM



# 四、数学基础4-凸优化

### 4.1、凸集

- 凸函数
- 凸集：
  - 定义：集合C内任意两点间的线段均在集合C内，则称集合C为 `凸集`
  - 凸函数 图像 上方的区域，一定是 凸集
- 拐点：求出二阶导数等于零，和二阶导数不存在的点。那么当改点两侧的符号相反时，改点是拐点；改点两侧符号相同时不是拐点。
- 仿射集（Affine set）
  - 定义：通过集合C中的任意两个不同点的直线仍然在集合C内，则称集合C为 `仿射集`。
  - 仿射集一定是无界的
  - 仿射集的例子：直线、平面、超平面
  - n维空间的n-1维仿射集为n-1维超平面。
  - 如果一个集合是仿射集，那么它一定是凸集
- 凸包：
  - 定义：集合C的所有点的凸组合形成的集合，叫做集合C的凸包。
- 超平面、半空间
- 二范式（点乘后开平方）
- 范数
- 绝对值：其实就是 一范式
- 范数球
- 范数锥
- 多面体：
  - 仿射集、射线、线段、半空间都是多面体
- 保持凸性的运算
  - 集合交运算
  - 仿射变换
    - 伸缩
    - 平移
    - 投影
  - 透视变换
  - 投射变换
    - 投射函数是透视函数和仿射函数的复合
- 分割超平面 
- 支撑超平面
  - 凸集边界上任意一点，均存在支撑超平面。
- Hessian矩阵（多元函数的二阶导的矩阵）
- 上境图、逐点上确界
- 亚图
- Jensen不等式（是几乎所有不等式的基础）
- 共轭函数
- Fenchel不等式
- Lagrange函数
- Lagrange对偶函数（一定是凹函数）
- KKT条件





# 六、

- 时域
- 频域
- 快速傅里叶变换FFT
- 奇异值分解(SVD)、SVD++。可做推荐系统
  - ALS
- 卷积



# 七、回归

- 样本的预测值是连续的称为回归(如：商品价格)

- 样本的预测值是离散的称为分类(如：投硬币)

- 线性回归：
  - 高斯分布

  - 最大似然估计MLE

  - 最小二乘法：最小二乘法（又称最小平方法）是一种数学优化技术，它通过最小化误差的平方和寻找数据的最佳函数匹配。

- Logistic回归

  - 分类问题的首选算法

- 实践中，有时候并不是特征越多，结果越好

- Lasso(L1正则化)

- Ridge(岭回归，L2正则化)

- 最小二乘法(样本服从高斯分布。样本的预测值与真实值的误差服从高斯分布$N(0, \sigma^2)$ )

  - 实际问题中，可能并不服从高斯分布，但是在机器学习中经常假定问题近似服从高斯分布。

  - 下式为线性回归的目标函数

  - $$
    J(\theta)=\cfrac{1}{2}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}=\cfrac{1}{2}(X{\theta}-y)^{T}(X{\theta}-y)
    $$

  - 对上式求导，使导数等于0得到下式

    - $\theta=(X^{T}X)^{-1}X^{T}y$

  - 若$X^{T}X$ 不可逆，或防止过拟合，增加  ${\lambda}$ 扰动

    - $\theta=(X^{T}X+{\lambda}I)^{-1}X^{T}y$        这就是Ridge（岭回归）

- 龙格现象

- GridSearchCV(CV：交叉验证)

- 广义逆矩阵(伪逆)：$X^{+}=(X^{T}X)^{-1}X^{T}$

- 梯度下降算法

- 随机梯度下降(SGD: stochastic gradient descent)
  - 经典的 随机梯度下降：指的是来了一个样本，就下降一次
  - 很常用的是mini-batch的随机梯度下降算法
  - 速度快

- 线性回归中：目标函数 $J(\theta)$ 是凸函数，所以局部最小值就是全局最小值(即，全局最优值)



# 八、回归实践

- 线性回归：求出的回归分界面，可以对样本是非线性的，只要对参数 $\theta$ 是线性的。



## 8.1、Coefficient of Determination(决定系数)

- 对于m个样本$(\vec{x_{1}},y_{1}),(\vec{x_{2}},y_{2}),\cdots,(\vec{x_{m}},y_{m})$
- 某模型的估计值为$(\vec{x_{1}},\hat{y}_{1}),(\vec{x_{2}},\hat{y}_{2}),\cdots,(\vec{x_{m}},\hat{y}_{m})$
- $\overline{y}$ 是样本值$y_{i}$ 的期望。
- 计算样本的`总平方和`$TSS$ (Total Sum of Squares): $TTS=\sum\limits_{i=1}^{m}(y_{i}-\overline{y})^{2}$

  - 即样本伪方差的m倍 $Var(Y)=TSS/m$

- 计算`残差平方和`$RSS$(Residual Sum of Squares): $RSS=\sum\limits_{i=1}^{m}(\hat{y}_{i}-y_{i})^{2}$

  - 注：`RSS`即`误差平方和`$SSE$ (Sum of Squares for Error)

- 定义

  - 决定系数(Coefficient of Determination): 
    $$
    R^{2}=1-\cfrac{RSS}{TSS}=1-\cfrac{\sum\limits_{i=1}^{m}(\hat{y}_{i}-y_{i})^{2}}{\sum\limits_{i=1}^{m}(y_{i}-\overline{y})^{2}}
    $$

  - $R^{2}$ 越大，你和效果越好。

  - $R^{2}$ 的最优值为1；若模型预测为随机值；$R^{2}$ 有可能为负数。

  - 若预测值恒为样本期望（即$\hat{y}=\overline{y}$），$R^{2}$ 为0。

- 亦可定义$ESS$ (Explained Sum of Squares)：$ESS=\sum\limits_{i=1}^{m}(\hat{y}_{i}-\overline{y})^{2}$

  - $TSS=ESS+RSS$
  - $ESS$又`称回归平方和`$SSR$ (Sum of Squares for Regression)

## 8.2、Logistic回归

- 据不加强线性回归

- 高斯核函数

- Logistic回归：二分类

  - sigmoid函数

  - $$
    g(x)=\cfrac{1}{1+e^{-x}}
    $$

    $$
    g'(x)=g(x)(1-g(x))
    $$

  - Logistic：$h_{\theta}(x)=g({\theta}^{T}x)=\cfrac{1}{1+e^{-{\theta}^{T}x}}$

  - Logistic回归没有解析解，只能通过数值计算

  - Logistic回归，是广义线性回归

  - Logistic回归参数的学习规则：

  - $$
    {\theta}_{j}={\theta}_{j}+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_{j}^{(i)}
    $$

    - 沿似然函数正梯度上升(求的是最大似然)

  - Softmax回归：多分类

  - 一个事件的几率odds，是指该事件发生的概率与该事件不发生的概率的比值。

  - 对数几率：logit函数

  - $$
    \log{it(p)}=
    \log{\cfrac{p}{1-p}}=
    \log{\cfrac{h_{\theta}(x)}{1-h_{\theta}(x)}}=
    \log
    {
    \begin{pmatrix}
    \cfrac{\cfrac{1}{1+e^{-{\theta}^{T}x}}}{\cfrac{e^{-{\theta}^{T}x}}{1+e^{-{\theta}^{T}x}}}
    \end{pmatrix}
    }=
    {\theta}^{T}x
    $$

  - 对数线性模型

- 负对数似然(NLL)：$loss(y_{i},\hat{y}_{i})=-l(\theta)$

  - 似然函数：$L({\theta})$
  - 对数似然：$l(\theta)=ln(L({\theta}))$  

- ROC
- AUC
- 奥卡姆剃刀原则    



# 九、决策树和随机森林

- CART(classification and regression tree)

- 熵

  - 即：$-\ln{p(x)}$ 在概率为$p(x)$下的期望

  $$
  H(X)=-\sum\limits_{x}p(x) \ln {p(x)}
  $$

- 条件熵
  $$
  H(Y \mid X)=H(X,Y)-H(X)=-\sum\limits_{x,y}p(x,y)\log{p(y \mid x)}=\sum\limits_{x}p(x)H(Y \mid X=x)
  $$

- 决策树：决策树学习采用的是自顶向下的递归方法，基本思想是以信息熵为度量构造一颗熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶子节点中的实例都属于同一类。

- 互信息：$I(X,Y)=H(X)+H(Y)-H(X,Y)=H(Y)-H(Y  \mid  X)$

  - Venn图

- 经验熵
- 经验条件熵
- 信息增益（即：特征$A$ 和数据集$D$的互信息）：
  - `信息增益` 表示得知特征 $A$ 的信息而使得类 $X$ 的信息的不确定性减少的程度。
  - 定义：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的`经验熵` $H(D)$ 与特征 $A$ 给定条件下 $D$ 的`经验条件熵` $H(D \mid A)$ 之差，即：
    - $g(D,A)=H(D)-H(D \mid A)$
    - 显然，这即为训练数据集$D$ 和特征 $A$ 的 `互信息`。



## 9.1、三种决策树学习方法

### 9.1.1、ID3算法

- 遍历所有特征
  - 选择信息增益最大的特征作为当前的分裂特征



### 9.1.2、C4.5算法

- 信息增益率：$g_{r}(D,A)=g(D,A)/H(A)$



### 9.1.3、CART

- Gini系数



### 9.1.4、小结

- 一个属性的信息增益(率)/gini系数越大，表明该属性对样本的熵减少的能力更强，这个属性使得数据由不确定性变成确定性的能力越强。



## 9.2、决策树的评价

- 纯节点

- 均节点

- 评价函数：

  - $$
    C(T)=\sum\limits_{t \in leaf}N_{t} \cdot H(t)
    $$

  - 设决策树有t个叶子节点

  - $N_{t}$：是叶子节点 t 中的样本个数

  - $H(t)$：是叶子节点 t 的熵

  - 由于该评价函数越小越好，所以，可以称之为 `"损失函数"` 。



## 9.3、

- 预剪枝：通过限制叶子节点的熵、通过限制叶子节点的样本个数。
- 后剪枝
  - 假定当前对以 $r$ 为跟的子树剪枝：
    - 剪枝后，只保留 $r$ 本身而删掉所有的叶子
  - 考察以 $r$ 为根的子树：
    - 剪枝后的损失函数：$C_{\alpha}(r)=C(r)+\alpha$
    - 剪枝前的损失函数：$C_{\alpha}(R)=C(R)+{\alpha}\cdot \left|R_{leaf} \right|$
      - $\left|R_{leaf} \right|$：以 $r$ 为跟的子树的叶子节点的个数。
    - 令二者相等，求得：$\alpha=\cfrac{C(r)-C(R)}{\left| R_{leaf} \right|-1}$
      - 二者相等，说明剪不剪枝都一样。
    - $\alpha$ 称为节点 $r$ 的 `剪枝系数`。
    - 算出所有的内部节点的 剪枝系数$\alpha$ ，从 $\alpha$ 最小的节点开始剪枝。
      - 每剪枝一次都得到一个$T_{k}$
      - 最后得到一系列树：$T_{0},\ T_{1},\ T_{2},\ \cdots , \ T_{root}$
      - 用测试集的数据来对这一系列树进行测试，看看哪棵树的损失值最小，就选哪棵树。

## 9.4、随机森林

- Bagging策略(有放回抽样)
  - bootstrap aggregation。
  - 从样本集中采样(有重复的)选出n个样本。
  - 在所有属性上，对这n个样本建立分类器(ID3、C4.5、CART、SVM、Logistic回归等)。
    - SVM、Logistic回归，都属于强分类器
  - 重复以上两步m次，即获得了m个分类器。
    - 得到了m个***弱分类器***(又称：基本分类器)
  - 将数据放在这m个分类器上，最后根据这m个分类器的投票结果（***强分类器***），决定数据属于哪一类。

- 采样
  - 对于一个分布 $f(x)$, 通过某种采样规则进行采样，有可能得到与原分布不同的 $g(x)$ 分布。

- 随机森林
  - 随机森林在bagging基础上做了修改。
    - 从样本集中用Bootstrap采样选出n个样本；
    - 从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树；
    - 重复以上两步m次，即建立和m棵CART决策树；
    - 这m个CART形成随机森林，通过投片表决结果，决定数据属于哪一类。
  - 随机森林中的随机，既有样本的随机，又有特征的随机。

## 9.5、样本不均衡的常用处理方法

- 假定样本数目A类比B类多，且严重不平衡：
  - A类欠采样Undersampling
    - 随机欠采样
    - A类分成若干子类，分别于B类进入ML模型
    - 基于聚类的A类分割
  - B类过采样Oversampling
    - 避免欠采样造成的信息丢失
    - 实践发现，对A欠采样的效果会比对B过采样的效果好，而且对A欠采样的计算速度快。
  - B类数据合成Synthetic Data Generation
    - 随机插值得到新样本
    - SMOTE(Synthetic Minority Over-sampling Technique)
  - 代价敏感学习Cose Sensitive Learning
    - 降低A类权值，提高B类权值
  - ***上述4种处理方法中：前3种是对样本进行处理。最后1种是对算法进行权值调整。***